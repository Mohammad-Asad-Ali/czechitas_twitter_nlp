{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjycJJ7j0ViB"
   },
   "source": [
    "---------------------------------------------------------------------\n",
    "# Twitter tweets - download all hashtag data\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "### There are limitations in using Tweepy for scraping tweets. \n",
    "### The standard API only allows you to retrieve tweets up to 7 days ago \n",
    "### and is limited to scraping 18,000 tweets per a 15 minute window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wN7aLC5H0ViF"
   },
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd \n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Twitter API deatils\n",
    "consumer_key= 'R4h8AHuGoIhHGaF8gqM9v7e09'\n",
    "consumer_secret= '84uSwytHNXQ7uj9AxziMdrFzsdCE2VUsqk67JJ2nZGYcnUe09M'\n",
    "access_token= '1130714178107400192-gsrzkNWAs3qhF0obWOgv251bEFh6AZ'\n",
    "access_token_secret= 'oVqNwg56RmgJCrCCQHT7lOMjB1TBGDqAMmVZ1I3y0fuqz'\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---- TODO---- #### ADD YOUR OWN PATH\n",
    "\n",
    "# my_path = '/Users/Petra_Kummerova/Desktop/Python/NLP/Czechitas/'\n",
    "# my_out_USE = \"my_out_USE.csv\"\n",
    "# my_out_USE_all_columns = \"my_out_USE_all_columns.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data cleaning \n",
    "\n",
    "------------------------------------------------------------\n",
    "#### Remove http and https link (find https until first space)\n",
    "------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tweets list for special characters\n",
    "def clean_tweets(tweets_list_all):\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    messages = [pattern.sub('', s) for s in tweets_list_all]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_dwnld(new_search,date_since):\n",
    "    tweets = tw.Cursor(api.search,\n",
    "                q=new_search,\n",
    "                # geocode=geocodes,\n",
    "                lang=\"en\",\n",
    "                since=date_since).items(1000)\n",
    "    tweets_list = []\n",
    "    tweets_authors = []\n",
    "    tweets_geo = []\n",
    "    tweets_date = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tweets_list = tweets_list + [tweet.text]\n",
    "        tweets_authors = tweets_authors + [tweet.user.name]\n",
    "        tweets_geo = tweets_geo + [tweet.place]\n",
    "        tweets_date = tweets_date + [tweet.created_at]\n",
    "    # clean tweets for special characters\n",
    "    messages = clean_tweets(tweets_list)\n",
    "    df_tweets = pd.DataFrame({'topic': new_search, 'tweet': messages, 'author': tweets_authors, 'geo': tweets_geo, 'twdate': tweets_date})\n",
    "    return df_tweets\n",
    "\n",
    "def search_tweets(search_word_list, date_since):\n",
    "\n",
    "    df_tweets_all = []\n",
    "    for search_word in tqdm(search_word_list):\n",
    "        print(f\"Currently searching for {search_word}\",end='\\r')\n",
    "        new_search = search_word + \" -filter:retweets\"\n",
    "        df_tweets = tweets_dwnld(new_search,date_since)\n",
    "        df_tweets_all.append(df_tweets)\n",
    "      #  print(\"end of one loop\")\n",
    "    return df_tweets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search term and the date_since date as variables\n",
    "search_word_list = [\"#Keboola\",\"#python\",\"#Tableau\",\"#Snowflake\",\"#R\"]\n",
    "geocodes = \"50.0755, 14.4378°, 500km\"\n",
    "# Prague 50.0755° N, 14.4378° E\n",
    "# search_word = \"#Keboola\" \n",
    "# new_search = search_word + \" -filter:retweets\"\n",
    "\n",
    "\n",
    "date_since = \"2021-08-20\" # only goes back max around 7 days anyways..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ydPuYic0ViI",
    "outputId": "eb9fc1f6-9aa9-4aab-d145-d135e66d422a"
   },
   "outputs": [],
   "source": [
    "# takes approx. 7 mins, depending on internet connection and speed\n",
    "\n",
    "df_tweets_all = search_tweets(search_word_list, date_since = date_since)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save into a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "z-2mBxvQ0ViJ",
    "outputId": "40b07bb1-da06-47c8-921f-58774587447e"
   },
   "outputs": [],
   "source": [
    "\n",
    "                                                                                                                                                                                                                   \n",
    "pickle.dump( df_tweets_all, open( \"df_tweets_all.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dictionary back from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_tweets_all_load = pickle.load( open( \"df_tweets_all.p\", \"rb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check one-by-one tweet categories (#Keboola\",\"#python\",\"#Tableau\",\"#Snowflake\",\"#R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_tweets_all_load[0])\n",
    "print(df_tweets_all_load[1])\n",
    "print(df_tweets_all_load[2])\n",
    "print(df_tweets_all_load[3])\n",
    "print(df_tweets_all_load[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_all_load_joined = pd.concat(df_tweets_all_load, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_all_load_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_all_load_joined['tweet_ID'] = df_tweets_all_load_joined.index\n",
    "df_tweets_all_load_joined.to_csv('df_tweets_all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "gGhDLBVc0ViJ",
    "outputId": "db12acfb-132a-4acb-ba5b-4d6de755c51e"
   },
   "outputs": [],
   "source": [
    "df_tweets_all_load_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the tweets from the dataframe\n",
    "messages = df_tweets_all_load_joined[\"tweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embeddings using USE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnrY_x3u0ViK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_sentence_embeddings(sentences_list, module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"):\n",
    "    \n",
    "    embed = hub.load(module_url)\n",
    "    sentence_embeddings = embed(sentences_list)\n",
    "    \n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might take up to 10 mins for first download/load, later on shall be ulpoaded within seconds from cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Pusl78LuCw6"
   },
   "outputs": [],
   "source": [
    "message_embeddings = generate_sentence_embeddings(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlQwZJo10ViM",
    "outputId": "cf28676d-addc-4fd8-f6b0-e80bb47c5130"
   },
   "outputs": [],
   "source": [
    "print(message_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_embeddings = message_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save into a pickle file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( message_embeddings, open( \"message_embeddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Load the dictionary back from the pickle file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_embeddings = pickle.load( open( \"message_embeddings.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering the embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### K-means clustering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "mVvFDw9w0ViM",
    "outputId": "fe9725a8-0e21-4496-e8e8-d73b76ec4a82"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert message embeddings to DataFrame with numeric values\n",
    "\n",
    "myX = pd.DataFrame(message_embeddings)\n",
    "X = myX\n",
    "X = X.apply(pd.to_numeric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering from text embeddings\n",
    "# number of clusters to have on average 30 tweets in each - can be tested for more options / better fit \n",
    "\n",
    "n_clusters_proportionate = int(round(len(X)/30,0))\n",
    "\n",
    "# apply k-means clustering \n",
    "\n",
    "kmeans_plus = KMeans(init='k-means++', n_clusters = n_clusters_proportionate, n_init=10)\n",
    "kmeans_plus.fit(X)\n",
    "cluster_labels = kmeans_plus.labels_\n",
    "\n",
    "# join labels with predicted clusters\n",
    "\n",
    "out = pd.DataFrame(columns = ['cluster_ID'])\n",
    "out['cluster_ID'] = cluster_labels \n",
    "out_merged = pd.merge(out, df_tweets_all_load_joined, left_index=True, right_index=True) #  to check output clusters merged with raw data\n",
    "print(out_merged)\n",
    "\n",
    "# add tweet_ID to mapoing table as output for Tableau\n",
    "out['tweet_ID'] = out.index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output to csv\n",
    "out.to_csv(my_path + \"out_cluster_twitter_ID_mapping.csv\", sep = ';')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some random clusters\n",
    "print(out_merged[out_merged.cluster_ID == 5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF to determine top topics within in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_preproc(text):\n",
    "\n",
    "    text = text.replace('.',' ')\n",
    "    text = text.replace('#','')\n",
    "    text = re.sub(r'\\s+',' ',re.sub(r'[^\\w \\s]','',text) ).lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7z-5N8b0pUm"
   },
   "outputs": [],
   "source": [
    "out_merged['cleaned'] = [tfidf_preproc(i) for i in out_merged.tweet] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949
    },
    "id": "77AfeJFs1MXr",
    "outputId": "ed920cd1-c444-4c31-be00-97813fa38cc0"
   },
   "outputs": [],
   "source": [
    "out_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataframe into clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goMtiIsD1mhk",
    "outputId": "15c74c3e-a5ab-4fe8-b788-0a21437ef75b"
   },
   "outputs": [],
   "source": [
    "gb = out_merged.groupby('cluster_ID')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zpmOtLO2qRI"
   },
   "outputs": [],
   "source": [
    "\n",
    "out_groups = [gb.get_group(x) for x in gb.groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unSh01sa3mhJ"
   },
   "outputs": [],
   "source": [
    "final = [''.join(i.cleaned) for i in out_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final[0])\n",
    "print(final[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4sQj9gV59vs"
   },
   "source": [
    "#### TF-IDF Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTNnHuidv27Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(final)\n",
    "names = vectorizer.get_feature_names()\n",
    "data = vectors.todense().tolist()\n",
    "# Create a dataframe with the results\n",
    "df = pd.DataFrame(data, columns=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = no. clusters, ncols = no. words in tfidf, matrix values = tfidf of each word in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OFNZV_bY6Cnv",
    "outputId": "a5f701be-a929-4b5e-9ded-85b9e1aa3bb6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all columns containing a stop word from the resultant dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIcZxqf-wRdw",
    "outputId": "f8b89a75-b974-4636-d6ee-ddb07d64c906"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "st = set(stopwords.words('english'))\n",
    "\n",
    "df = df[filter(lambda x: x not in list(st) , df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check top 10 topics within each cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_X_topics(df, X_topics):\n",
    "\n",
    "    out_cluster_topics = pd.DataFrame(columns = ['tfidf', 'topics','cluster_ID'])\n",
    "\n",
    "    n = X_topics;\n",
    "    for i in df.iterrows():\n",
    "        print(i[1].sort_values(ascending=False)[:n])\n",
    "        my_df = pd.DataFrame(i[1].sort_values(ascending=False)[:n])\n",
    "        cluster_id = list(my_df)[0]\n",
    "        my_df['topics'] = my_df.index\n",
    "        my_df.columns = ['tfidf', 'topics']\n",
    "        my_df['cluster_ID'] = cluster_id\n",
    "        out_cluster_topics = pd.concat([out_cluster_topics,my_df], ignore_index=True)\n",
    "\n",
    "    return out_cluster_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join with full data to export to Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_cluster_topics = add_top_X_topics(df,10)\n",
    "out_cluster_topics.to_csv(my_path + 'out_cluster_metadata.csv', sep = ';')\n",
    "\n",
    "# check out joined in master table with raw data\n",
    "out_all_columns = pd.merge(out_merged, out_cluster_topics, right_on = 'cluster_ID', left_on = 'cluster_ID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction using PCA for cluster plotting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFLsX3NN6_k0",
    "outputId": "c268d705-c8dc-4b59-f297-b3cf2b9ee032"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pca_coordinates(original_vectors_array, num_components = 2):\n",
    "    X = original_vectors_array\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca_embeddings = pca.fit_transform(X)\n",
    "    pca_coords = pd.DataFrame(pca_embeddings,\n",
    "                          columns=['x','y'])\n",
    "    return pca_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949
    },
    "id": "00VZLf5H9JF8",
    "outputId": "5a04733e-ed96-4c7b-e1b1-ba288d53eeae"
   },
   "outputs": [],
   "source": [
    "out_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyP7JtJQ978Q"
   },
   "outputs": [],
   "source": [
    "out_plot = out_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce 512-dim to 2-dim to be able to visualize in 2D chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_coords = generate_pca_coordinates(message_embeddings, num_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w47T5YxP-GHZ"
   },
   "outputs": [],
   "source": [
    "out_plot['x'] = pca_coords['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2hY_XAQ-yH2"
   },
   "outputs": [],
   "source": [
    "out_plot['y'] = pca_coords['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_plot_pca = out_plot[['x','y','cluster_ID']]\n",
    "out_plot_pca.to_csv(my_path + 'out_plot_pca.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the 2D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "tgjCSx_89nno",
    "outputId": "eafd9553-7436-481b-b77d-7f491be9b047"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm  # to have nicer and more colors\n",
    "\n",
    "\n",
    "N = len(out_plot)\n",
    "x = out_plot['x']\n",
    "y = out_plot['y']\n",
    "colors = out_plot['cluster_ID']/100 # divide by 100 to have decimal numbners to have broader scale of colors\n",
    "\n",
    "plt.scatter(x, y, c=cm.rainbow(colors), alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mBKlcYsHlIp"
   },
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "TD433EQkISMs",
    "outputId": "f1c755d5-8d47-4bd3-d9a4-719e008dacfd"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#linked = linkage(np.array(X), 'single')\n",
    "linked = linkage(np.array(X), 'ward')\n",
    "labelList = out.index.values\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))  \n",
    "dendrogram(linked,  \n",
    "            orientation='top',\n",
    "            labels=labelList,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------- Tableau Input -------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Raw data (location, date, http link, twitter text)  + ID twitter + added hyperlinks with https = df_tweets_all.csv\n",
    "\n",
    "### 2. ID twitter + ID cluster = out_cluster_twitter_ID_mapping.csv\n",
    "\n",
    "### 3. ID cluster + Custer data (top 10 keywords) + ID keyword  = out_cluster_metadata.csv\n",
    "\n",
    "### 4. ID cluster + x + y + ID Twitter (optional) = out_plot_pca.csv\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
